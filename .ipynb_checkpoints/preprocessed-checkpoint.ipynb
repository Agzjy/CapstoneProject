{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jR13h9bd4fiB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import scipy.stats as stats\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UQ4XnwMk4ffe"
   },
   "outputs": [],
   "source": [
    "#import dataset\n",
    "train = pd.read_csv('~/Desktop/trainProcessed.csv')\n",
    "validate = pd.read_csv('~/Desktop/validateProcessed.csv')\n",
    "test = pd.read_csv('~/Desktop/testProcessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vQsUgiNy4fYG"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/release_conditions.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mj/nswrhp457cg6nstcxp_92m2h0000gn/T/ipykernel_86131/1690107563.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpathology_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/release_conditions.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mevidences_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/release_evidences.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/release_conditions.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "pathology_dict = json.load(open('/content/drive/MyDrive/release_conditions.json', 'r'))\n",
    "evidences_dict = json.load(open('/content/drive/MyDrive/release_evidences.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbIZB8Td4rJd"
   },
   "outputs": [],
   "source": [
    "def check_typo(df):\n",
    "    for n, i in df.iterrows():\n",
    "        if i['PATHOLOGY'] not in pathology_dict.keys():\n",
    "            print(f\"{n} has a typo in pathology: {i['PATHOLOGY']}\")\n",
    "            df.drop[n]\n",
    "        for evidence in i['EVIDENCES']:\n",
    "            evidence_split = evidence.split('_@_')\n",
    "            question = evidence_split[0]\n",
    "            if question not in evidences_dict.keys():\n",
    "                print(f'{n} has a typo in question: {evidence}')\n",
    "                df.drop[n]\n",
    "                continue\n",
    "            if len(evidence_split) > 1:\n",
    "                value = evidence_split[1]\n",
    "                if value not in evidences_dict[question][\"possible-values\"] \\\n",
    "                    and value not in [str(e) for e in evidences_dict[question][\"possible-values\"]]:\n",
    "                    print(f'{n} has a typo in value: {evidence}')\n",
    "                    df.drop[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyrS7qaJ4tS5"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(df):\n",
    "    unique_items = set(item for sublist in df['EVIDENCES'] for item in sublist)\n",
    "    encoded_evidence = pd.DataFrame(0, index=df.index, columns=list(unique_items))\n",
    "    for item in unique_items:\n",
    "        encoded_evidence[item] = df['EVIDENCES'].apply(lambda x: item in x)\n",
    "    return encoded_evidence.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQ1KdgKU4vFt"
   },
   "outputs": [],
   "source": [
    "def readingData(df):\n",
    "  # Get an overview of data\n",
    "  print(\"DataFrame shape: \"+str(df.shape))\n",
    "\n",
    "  # Deal with Nan value and Duplicate\n",
    "  print(df.isna().sum())\n",
    "  df.dropna()\n",
    "  print('Total Duplicate: '+str(df.duplicated().sum()))\n",
    "  df.drop_duplicates()\n",
    "\n",
    "  # Turn variable \"EVIDENCES\" from string to list\n",
    "  evidences = []\n",
    "  for evidence_row in df['EVIDENCES']:\n",
    "    evidences.append([evidence.strip(\" ''\") for evidence in evidence_row.strip(\"[]\").split(\",\") ])\n",
    "  df['EVIDENCES'] = evidences\n",
    "\n",
    "  # Drop Outlier\n",
    "  df = df[(df['AGE'] <= 120) & (df['AGE'] >= 0)]\n",
    "\n",
    "  # Check Typo\n",
    "  check_typo(df)\n",
    "\n",
    "  # Data Encoding\n",
    "  df['SEX'] = df['SEX'].map({'M': 0, 'F': 1})\n",
    "  encoded_df = one_hot_encode(df)\n",
    "  df = df.drop('EVIDENCES', axis=1).join(encoded_df)\n",
    "  print(df.head())\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9XyB_YA4vAs"
   },
   "outputs": [],
   "source": [
    "train=readingData(train)\n",
    "validate=readingData(validate)\n",
    "test=readingData(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCK-TnRP44GU"
   },
   "outputs": [],
   "source": [
    "all_columns = pd.Series(list(train.columns) + list(validate.columns) + list(test.columns)).drop_duplicates()\n",
    "train = train.reindex(columns=all_columns, fill_value=0)\n",
    "validate = validate.reindex(columns=all_columns, fill_value=0)\n",
    "test = test.reindex(columns=all_columns, fill_value=0)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
